# Implementation-and-Optimization-of-GPT-2-Model
This repository contains Implementation and Optimization of GPT-2 Model through transformer architecture and modifying its structure for improved performance further it includes developing an efficient training loop and implementation of distributed training applicable across multiple GPUs.

#TASK-1
#GPT-2 Model & Checkpoints 

At first we start by implementing the GPT-2 model. We will create a basic version of the GPT-2-small model with 125 million parameters.This implementation is a simplified version for demonstration purposes, and we may need to adapt it based on our specific requirements.

